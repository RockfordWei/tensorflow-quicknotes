{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.data import Dataset\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "INPUT_FEATURE = 'pixels'\n",
    "SHAPE = 784\n",
    "def parse_labels_and_features(dataset):\n",
    "  \"\"\"Extracts labels and features.\n",
    "  \n",
    "  This is a good place to scale or transform the features if needed.\n",
    "  \n",
    "  Args:\n",
    "    dataset: A Pandas `Dataframe`, containing the label on the first column and\n",
    "      monochrome pixel values on the remaining columns, in row major order.\n",
    "  Returns:\n",
    "    A `tuple` `(labels, features)`:\n",
    "      labels: A Pandas `Series`.\n",
    "      features: A Pandas `DataFrame`.\n",
    "  \"\"\"\n",
    "  labels = dataset[0]\n",
    "\n",
    "  # DataFrame.loc index ranges are inclusive at both ends.\n",
    "  features = dataset.loc[:,1:784]\n",
    "  # Scale the data to [0, 1] by dividing out the max value, 255.\n",
    "  features = features / 255\n",
    "\n",
    "  return labels, features\n",
    "\n",
    "def construct_feature_columns():\n",
    "  \"\"\"Construct the TensorFlow Feature Columns.\n",
    "\n",
    "  Returns:\n",
    "    A set of feature columns\n",
    "  \"\"\" \n",
    "  \n",
    "  # There are 784 pixels in each image.\n",
    "  return set([tf.feature_column.numeric_column(INPUT_FEATURE, shape=SHAPE)])\n",
    "\n",
    "def create_predict_input_fn(features, labels, batch_size):\n",
    "  \"\"\"A custom input_fn for sending mnist data to the estimator for predictions.\n",
    "\n",
    "  Args:\n",
    "    features: The features to base predictions on.\n",
    "    labels: The labels of the prediction examples.\n",
    "\n",
    "  Returns:\n",
    "    A function that returns features and labels for predictions.\n",
    "  \"\"\"\n",
    "  def _input_fn():\n",
    "    raw_features = {INPUT_FEATURE: features.values}\n",
    "    raw_targets = np.array(labels)\n",
    "    \n",
    "    ds = Dataset.from_tensor_slices((raw_features, raw_targets)) # warning: 2GB limit\n",
    "    ds = ds.batch(batch_size)\n",
    "    \n",
    "        \n",
    "    # Return the next batch of data.\n",
    "    feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
    "    return feature_batch, label_batch\n",
    "\n",
    "  return _input_fn\n",
    "\n",
    "def inference(examples, targets, batch_size, hidden_units, model_dir):\n",
    "    predict_training_input_fn = create_predict_input_fn(examples, targets, batch_size)\n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "      feature_columns=construct_feature_columns(),\n",
    "      hidden_units=hidden_units,\n",
    "      n_classes=10,\n",
    "      config=tf.estimator.RunConfig(keep_checkpoint_max=1),\n",
    "      model_dir=model_dir\n",
    "    )\n",
    "    predictions = list(classifier.predict(input_fn=predict_training_input_fn))\n",
    "    class_id = np.array([item['class_ids'][0] for item in predictions])\n",
    "    return class_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    7\n",
      "Name: 0, dtype: int64\n",
      "   1    2    3    4    5    6    7    8    9    10  ...   775  776  777  778  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0  0.0  0.0   \n",
      "\n",
      "   779  780  781  782  783  784  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[1 rows x 784 columns]\n",
      "test prediction [7]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mnist_test_dataframe = pd.read_csv(\n",
    "  \"mnist_test.csv\",\n",
    "  sep=\",\",\n",
    "  header=None)\n",
    "\n",
    "test_targets, test_examples = parse_labels_and_features(mnist_test_dataframe)\n",
    "single_target = test_targets.head(1)\n",
    "single_example = test_examples.head(1)\n",
    "print(single_target)\n",
    "print(single_example)\n",
    "\n",
    "test_class = inference(examples=single_example, targets=single_target, batch_size=1, hidden_units=[128, 128, 128], model_dir='models/mnist.ckpt')\n",
    "print(\"test prediction\", test_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
